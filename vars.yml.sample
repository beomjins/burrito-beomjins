---
### common
# deploy_ssh_key: (boolean) create ssh keypair and copy it to other nodes.
# default: false
deploy_ssh_key: false

### define network interface names
# set overlay_iface_name to null if you do not want to set up overlay network.
# then, only provider network will be set up.
svc_iface_name: eth0
mgmt_iface_name: eth1
provider_iface_name: eth2
overlay_iface_name: eth3
storage_iface_name: eth4

### ntp
# Specify time servers for control nodes.
# You can use the default ntp.org servers or time servers in your network.
# If servers are offline and there is no time server in your network,
#   set ntp_servers to empty list.
#   Then, the control nodes will be the ntp servers for other nodes.
# ntp_servers: []
ntp_servers:
  - 0.pool.ntp.org
  - 1.pool.ntp.org
  - 2.pool.ntp.org

### keepalived
keepalived_vip: "192.168.21.90"

### storage
# storage backends: ceph and(or) netapp
# If there are multiple backends, the first one is the default backend.
storage_backends:
  - netapp
  - ceph

# ceph: set ceph configuration in group_vars/all/ceph_vars.yml
# netapp: set netapp configuration in group_vars/all/netapp_vars.yml

### MTU setting
calico_mtu: 1500
openstack_mtu: 1500

### neutron
# is_ovs: set false for linuxbridge(default), set true for openvswitch 
is_ovs: false
bgp_dragent: false


###################################################
## Do not edit below if you are not an expert!!!  #
###################################################
# vault passwords
ansible_password: "{{ vault_ssh_password }}"
ansible_become_password: "{{ vault_sudo_password }}"

# offline installation flag - default false
# do not change this variable to true
# if you want offline installation, just add offline_vars.yml in extra-vars
# it will override the value - refer to run.sh script.
offline: false

### ntp
mgmt_netmask: "{{ hostvars[inventory_hostname]['ansible_' + mgmt_iface_name].ipv4.network }}/{{ hostvars[inventory_hostname]['ansible_' + mgmt_iface_name].ipv4.netmask }}"
ntp_allowed_cidr: "{{ mgmt_netmask | ansible.utils.ipaddr('net') }}"

### keepalived role variables
keepalived_interface: "{{ mgmt_iface_name }}"
# make keepalived service vip optional.
# keepalived service interface vip for future use (default: no setup).
keepalived_interface_svc: "{{ svc_iface_name }}"
# if you want to set up VIP on service network, assign VIP address here
keepalived_vip_svc: ""

### ceph-ansible
# ceph network cidr - recommend the same cidr for public/cluster networks.
storage_netmask: "{{ hostvars[inventory_hostname]['ansible_' + storage_iface_name].ipv4.network }}/{{ hostvars[inventory_hostname]['ansible_' + storage_iface_name].ipv4.netmask }}"
public_network: "{{ storage_netmask | ansible.utils.ipaddr('net') }}"
cluster_network: "{{ public_network }}"
dashboard_enabled: false
configure_firewall: false
ceph_origin: repository
ceph_repository: community
ceph_repository_type: cdn
ceph_stable_release: quincy
osd_objectstore: bluestore
docker_pull_timeout: "60s"

# set the size, min_size, and single_osd_node
ceph_pool_default_size: "{% if (groups['osds']|length) > 3 %}3{% else %}2{% endif %}"
ceph_pool_default_min_size: "{% if (groups['osds']|length) > 3 %}2{% else %}1{% endif %}"
single_osd_node: "{{ ((groups['osds']|length) == 1)|ternary('true', 'false') }}"

ceph_conf_overrides:
  global:
    auth_allow_insecure_global_id_reclaim: false
    mon_allow_pool_delete: true
    osd_pool_default_size: "{{ ceph_pool_default_size }}"
    osd_pool_default_min_size: "{{ ceph_pool_default_min_size }}"
    osd_crush_chooseleaf_type: "{{ single_osd_node | ternary(0, 1) }}"

# rgw
radosgw_frontend_type: beast
radosgw_frontend_port: 7480

# openstack/k8s pools
openstack_config: true
kube_pool:
  name: "kube"
  application: "rbd"
  target_size_ratio: 0.2
openstack_glance_pool:
  name: "images"
  application: "rbd"
  target_size_ratio: 0.3
openstack_cinder_pool:
  name: "volumes"
  application: "rbd"
  target_size_ratio: "{{ enable_cinder_backup | ternary(0.3, 0.4) }}"
openstack_cinder_backup_pool:
  name: "backups"
  application: "rbd"
  target_size_ratio: 0.1
openstack_nova_vms_pool:
  name: "vms"
  application: "rbd"
  target_size_ratio: 0.1

openstack_basic_pools:
  - "{{ kube_pool }}"
  - "{{ openstack_glance_pool }}"
  - "{{ openstack_cinder_pool }}"
  - "{{ openstack_nova_vms_pool }}"

openstack_pools: "{% if enable_cinder_backup %}{{ openstack_basic_pools + [openstack_cinder_backup_pool] }}{% else %}{{ openstack_basic_pools }}{% endif %}"

openstack_keys:
  - { name: client.kube, caps: { mon: "profile rbd", osd: "profile rbd pool={{ kube_pool.name }}", mgr: "profile rbd pool={{ kube_pool.name }}" }, mode: "0600" }
  - { name: client.glance, caps: { mon: "profile rbd", osd: "profile rbd pool={{ openstack_cinder_pool.name }}, profile rbd pool={{ openstack_glance_pool.name }}"}, mode: "0600" }
  - { name: client.cinder, caps: { mon: "profile rbd", osd: "profile rbd pool={{ openstack_cinder_pool.name }}, profile rbd pool={{ openstack_nova_pool.name }}, profile rbd pool={{ openstack_glance_pool.name }}"}, mode: "0600" }
  - { name: client.cinder-backup, caps: { mon: "profile rbd", osd: "profile rbd pool={{ openstack_cinder_backup_pool.name }}"}, mode: "0600" }
  - { name: client.openstack, caps: { mon: "profile rbd", osd: "profile rbd pool={{ openstack_glance_pool.name }}, profile rbd pool={{ openstack_nova_pool.name }}, profile rbd pool={{ openstack_cinder_pool.name }}, profile rbd pool={{ openstack_cinder_backup_pool.name }}"}, mode: "0600" }

# clients
copy_admin_key: true

### kubespray
# default pod replicas == # of controllers
pod:
  replicas: "{{ groups['controller-node']|length }}"
upstream_dns_servers:
  - 8.8.8.8
kube_image_repo: "registry.k8s.io"
gcr_image_repo: "gcr.io"
github_image_repo: "ghcr.io"
docker_image_repo: "docker.io"
quay_image_repo: "quay.io"
download_run_once: false
bin_dir: /usr/bin
kube_version: v1.24.8
kube_proxy_strict_arp: true
podsecuritypolicy_enabled: true
kubeconfig_localhost: true
kubectl_localhost: true
auto_renew_certificates: true
helm_enabled: true
cert_manager_enabled: true
cert_manager_affinity:
 nodeAffinity:
   preferredDuringSchedulingIgnoredDuringExecution:
   - weight: 100
     preference:
       matchExpressions:
       - key: node-role.kubernetes.io/control-plane
         operator: In
         values:
         - ""
preinstall_selinux_state: disabled
kube_pods_subnet: 10.233.64.0/18
kube_apiserver_enable_admission_plugins:
  - AlwaysPullImages
  - EventRateLimit
  - NodeRestriction
  - PodSecurityPolicy
  - ServiceAccount
kube_apiserver_admission_control_config_file: true
kube_apiserver_admission_event_rate_limits:
  limit1:
    type: Namespace
    qps: 500
    burst: 5000
    cache_size: 2000
  limit2:
    type: User
    qps: 500
    burst: 5000
kube_encrypt_secret_data: true
  
# calico
calico_network_backend: bird
calico_ipip_mode: 'Never'
calico_vxlan_mode: 'Never'
# Registry deployment
registry_enabled: true
registry_namespace: kube-system
registry_storage_class: "{{ storage_backends[0] }}"
registry_disk_size: "30Gi"
registry_service_type: "NodePort"
registry_service_nodeport: "32680"
seed_registry_port: "5000"
containerd_insecure_registries:
  "local_registry": "{{ keepalived_vip }}:{{ registry_service_nodeport }}"
  "seed_registry": "{{ keepalived_vip }}:{{ seed_registry_port }}"

### burrito variables
# haproxy role variables
ceph_rgw_port: 7480

# kubespray/roles/burrito.openstack/.../ceph_provisioners.yml.j2
ceph_public_network: "{{ public_network }}"
ceph_cluster_network: "{{ cluster_network }}"

# storageclass_name is used by ingress, mariadb, rabbitmq, glance, nova
# Default: the first one of storage_backends
storageclass_name: "{{ storage_backends[0] }}"

# ingress
ingress:
  volume_size: "100Gi"

# mariadb
mariadb:
  volume_size: "30Gi"
  admin_password: "{{ vault_mariadb_root_password }}"

# rabbitmq
rabbitmq:
  volume_size: "768Mi"
  password: "{{ vault_rabbitmq_openstack_password }}"

# keystone
os_admin_password: "{{ vault_openstack_admin_password }}"
keystone:
  password: "{{ vault_keystone_password }}"

# glance
glance:
  stores: "{{ storage_backends|map('extract', {'ceph': 'rbd', 'netapp': 'file'})|list|first }}"
  default_store: "{{ storage_backends|map('extract', {'ceph': 'rbd', 'netapp': 'file'})|list|unique|first }}"
  password: "{{ vault_glance_password }}"
  nginx:
    proxy_body_size: "102400M"
    proxy_read_timeout: "1800"
  volume_size: "500Gi"

# placement
placement:
  password: "{{ vault_placement_password }}"

# libvirt
ceph_secret_uuid: "{{ vault_ceph_secret_uuid }}"

# neutron
neutron_ml2_plugin: "{{ is_ovs|ternary('openvswitch', 'linuxbridge') }}"
neutron:
  tunnel: "{{ overlay_iface_name }}"
  tunnel_compute: "{{ overlay_iface_name }}"
  password: "{{ vault_neutron_password }}"
ovs_dvr: "{{ is_ovs|ternary(true, false) }}"
ovs_provider:
  - name: external
    bridge: br-ex
    iface: "{{ provider_iface_name }}"
    vlan_ranges: ""
lb_iface_mappings:
  - "external:{{ provider_iface_name }}"

# nova
nova:
  vncserver_proxyclient_interface: "{{ mgmt_iface_name }}"
  hypervisor_host_interface: "{{ mgmt_iface_name }}"
  libvirt_live_migration_interface: "{{ mgmt_iface_name }}"
  password: "{{ vault_nova_password }}"
  volume_size: "500Gi"

nova_uid: 42424
nova_gid: 42424

# cinder
# cinder backup deployment(default): false
enable_cinder_backup: false
cinder_backends: "{% for s in storage_backends %}{% if s == 'ceph' %}rbd1{% endif %}{% if s == 'netapp' %}{% for n in netapp %}{{ n.name }}{% if not loop.last %},{% endif %}{% endfor %}{% endif %}{% if not loop.last %},{% endif %}{% endfor %}"
cinder:
  enabled_backends: "{{ cinder_backends }}"
  default_volume_type: "{{ cinder_backends|split(',')|first }}"
  password: "{{ vault_cinder_password }}"

# horizon
horizon:
  password: "{{ vault_horizon_password }}"
  timezone: "Asia/Seoul"

# barbican
barbican:
  password: "{{ vault_barbican_password }}"
  kek: "{{ vault_barbican_kek | b64encode }}"

### btx
btx:
  version: "1.1.1"
  pvc:
    size: "100Gi"
...
